llm.api_key='your-openai-key'
log_db.connection_string='log_db.sqlite3'

# exchange with the IP of your target VM
conn.host='enter-the-private-ip-of-some-vm.local'
conn.hostname='the-hostname-of-the-vm-used-for-root-detection'
conn.port=2222

# exchange with the user for your target VM
conn.username='bob'
#To just use keyauth only, use '' with no space for conn.password 
#Otherwise, insert the password for instance here
conn.password='secret'
#To just use username and password auth only, use '' with no space for conn.keyfilename
#Otherwise, insert the filepath for the keyfile here (for example, '/home/bob/.ssh/sshkey.rsa')
conn.keyfilename=''

# which LLM model to use (can be anything openai supports, or if you use a custom llm.api_url, anything your api provides for the model parameter
llm.model='gpt-3.5-turbo'
llm.context_size=16385

# how many rounds should this thing go?
max_turns = 20

# The following four parameters are only relevant for the usecase rag
# rag_database_folder_name: Name of the folder where the vector store will be saved.
# rag_embedding: The name of the embedding model used. Currently only OpenAI api supported.
# openai_api_key: API key that is used for the embedding model.
# rag_return_token_limit: The upper bound for the RAG output.
rag_database_folder_name = "vetorDB"
rag_embedding = "text-embedding-3-small"
openai_api_key = 'your-openai-key'
rag_return_token_limit = 1000
